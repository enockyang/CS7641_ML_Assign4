{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 0.0, False, {'prob': 0.3333333333333333})\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\")\n",
    "env.reset()                    \n",
    "print(env.step(2))\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0.0, False, {'prob': 0.3333333333333333})\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake8x8-v1\")\n",
    "env.reset()                    \n",
    "print(env.step(2))\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space:  Discrete(4)\n",
      "Observation space:  Discrete(64)\n"
     ]
    }
   ],
   "source": [
    "print(\"Action space: \", env.action_space)\n",
    "print(\"Observation space: \", env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.nS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 12, 0.0, False), (0.3333333333333333, 5, 0.0, False)]\n"
     ]
    }
   ],
   "source": [
    "print(env.P[4][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, max_iterations=100000, lmbda=0.99, theta=1e-20):\n",
    "    env.reset()\n",
    "    stateValue = np.zeros(env.nS)\n",
    "    bestPolicy = np.zeros(env.nS)\n",
    "    for i in range(max_iterations):\n",
    "        newStateValue = np.copy(stateValue)\n",
    "        for state in range(env.nS):\n",
    "            action_values = []      \n",
    "            for action in range(env.nA):\n",
    "                next_state_rewards = []\n",
    "                for next_sr in env.P[state][action]:\n",
    "                    prob, next_state, reward, done = next_sr\n",
    "                    next_state_rewards.append(prob * (reward + lmbda*newStateValue[next_state]))      #the value of each action\n",
    "                action_values.append(np.sum(next_state_rewards))\n",
    "            stateValue[state] = max(action_values)  #update the value of the state\n",
    "            bestPolicy[state] = action_values.index(max(action_values))\n",
    "        \n",
    "        if (np.sum(np.fabs(newStateValue-stateValue)) <= theta):\n",
    "            print(\"Value-iteration converged at iteration #\",i)\n",
    "            break\n",
    "    \n",
    "    return stateValue, bestPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value-iteration converged at iteration # 1125\n"
     ]
    }
   ],
   "source": [
    "env.reset()  \n",
    "v, p = value_iteration(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.41464036 0.42720522 0.44614822 0.46832037 0.49244371 0.51656983\n",
      " 0.53526151 0.54097522 0.41168642 0.42120783 0.43749572 0.45838855\n",
      " 0.48324013 0.51353178 0.54576786 0.55736841 0.39675209 0.39384054\n",
      " 0.37549627 0.         0.42167799 0.49381921 0.56121207 0.5858589\n",
      " 0.36927228 0.35298254 0.30653123 0.20040371 0.30075275 0.\n",
      " 0.56901589 0.62825904 0.33266395 0.29137537 0.19730918 0.\n",
      " 0.28929026 0.36195181 0.53481945 0.68969732 0.30613635 0.\n",
      " 0.         0.08627639 0.2139326  0.27271394 0.         0.77203552\n",
      " 0.2888856  0.         0.05769641 0.04751102 0.         0.25052148\n",
      " 0.         0.87776874 0.28038897 0.20081512 0.12732657 0.\n",
      " 0.23959086 0.48644206 0.7371033  0.        ]\n",
      "[[3. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [3. 3. 3. 3. 3. 2. 2. 1.]\n",
      " [3. 3. 0. 0. 2. 3. 2. 1.]\n",
      " [3. 3. 3. 1. 0. 0. 2. 2.]\n",
      " [0. 3. 0. 0. 2. 1. 3. 2.]\n",
      " [0. 0. 0. 1. 3. 0. 0. 2.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 2.]\n",
      " [0. 1. 0. 0. 1. 2. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(v)\n",
    "print(np.reshape(p,(8,8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8 1e-05\n",
      "Value-iteration converged at iteration # 38\n",
      "0.8 1e-10\n",
      "Value-iteration converged at iteration # 79\n",
      "0.8 1e-20\n",
      "Value-iteration converged at iteration # 147\n",
      "0.9 1e-05\n",
      "Value-iteration converged at iteration # 74\n",
      "0.9 1e-10\n",
      "Value-iteration converged at iteration # 158\n",
      "0.9 1e-20\n",
      "Value-iteration converged at iteration # 266\n",
      "0.95 1e-05\n",
      "Value-iteration converged at iteration # 122\n",
      "0.95 1e-10\n",
      "Value-iteration converged at iteration # 262\n",
      "0.95 1e-20\n",
      "Value-iteration converged at iteration # 426\n",
      "0.99 1e-05\n",
      "Value-iteration converged at iteration # 288\n",
      "0.99 1e-10\n",
      "Value-iteration converged at iteration # 620\n",
      "0.99 1e-20\n",
      "Value-iteration converged at iteration # 995\n"
     ]
    }
   ],
   "source": [
    "for lmbda in [0.8,0.9,0.95,0.99]:\n",
    "    for theta in [1e-5, 1e-10, 1e-20]:\n",
    "        print(lmbda,theta)\n",
    "        value_iteration(env,lmbda=lmbda,theta=theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8612\n"
     ]
    }
   ],
   "source": [
    "e = 0\n",
    "for i_episode in range(10000):\n",
    "    c = env.reset()\n",
    "    while True:\n",
    "        c,reward,done,info = env.step(int(p[c]))\n",
    "        #time.sleep(0.5)\n",
    "        #env.render()\n",
    "        if done:\n",
    "            if reward == 1:\n",
    "                e +=1\n",
    "            break\n",
    "\n",
    "print(e/10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(policy, lmbda=0.99, theta=1e-5):\n",
    "    stateValue = np.zeros(env.nS)\n",
    "    i = 0\n",
    "    while True:\n",
    "        newStateValue = np.copy(stateValue)\n",
    "        for state in range(env.nS):\n",
    "            action = policy[state]\n",
    "            stateValue[state] = sum([prob * (reward + lmbda*newStateValue[next_state]) for prob, next_state, reward, done in env.P[state][action]])\n",
    "        i += 1\n",
    "        if (np.sum(np.fabs(newStateValue-stateValue)) <= theta):\n",
    "            print(\"compute_value_function at iteration #\",i)\n",
    "            break\n",
    "    return stateValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_policy(stateValue, lmbda=0.99):\n",
    "    policy = np.zeros(env.nS)\n",
    "    for state in range(env.nS):\n",
    "        action_values = np.zeros(env.nA)\n",
    "        for action in range(env.nA):\n",
    "            for next_sr in env.P[state][action]:\n",
    "                    prob, next_state, reward, done = next_sr\n",
    "                    action_values[action] += prob * (reward + lmbda*stateValue[next_state])\n",
    "        policy[state] = np.argmax(action_values)\n",
    "    return policy    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iterations(env, max_iterations=100000, lmbda=0.99,):\n",
    "    env.reset()  \n",
    "    random_policy = np.zeros(env.nS)\n",
    "    for i in range(max_iterations):\n",
    "        value = evaluate(random_policy,lmbda)\n",
    "        new_policy = extract_policy(value,lmbda)\n",
    "        if (np.all(random_policy == new_policy)):\n",
    "            print(\"policy-iteration converged at iteration #\",i)\n",
    "            break\n",
    "        random_policy = new_policy\n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute_value_function at iteration # 22\n",
      "compute_value_function at iteration # 382\n",
      "compute_value_function at iteration # 399\n",
      "compute_value_function at iteration # 409\n",
      "compute_value_function at iteration # 334\n",
      "compute_value_function at iteration # 340\n",
      "compute_value_function at iteration # 346\n",
      "compute_value_function at iteration # 357\n",
      "compute_value_function at iteration # 377\n",
      "compute_value_function at iteration # 381\n",
      "compute_value_function at iteration # 386\n",
      "policy-iteration converged at iteration # 10\n"
     ]
    }
   ],
   "source": [
    "p_new = policy_iterations(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 2. 2. 2. 2. 2. 2. 2. 3. 3. 3. 3. 3. 2. 2. 1. 3. 3. 0. 0. 2. 3. 2. 1.\n",
      " 3. 3. 3. 1. 0. 0. 2. 2. 0. 3. 0. 0. 2. 1. 3. 2. 0. 0. 0. 1. 3. 0. 0. 2.\n",
      " 0. 0. 1. 0. 0. 0. 0. 2. 0. 1. 0. 0. 1. 2. 1. 0.]\n",
      "[3. 2. 2. 2. 2. 2. 2. 2. 3. 3. 3. 3. 3. 2. 2. 1. 3. 3. 0. 0. 2. 3. 2. 1.\n",
      " 3. 3. 3. 1. 0. 0. 2. 2. 0. 3. 0. 0. 2. 1. 3. 2. 0. 0. 0. 1. 3. 0. 0. 2.\n",
      " 0. 0. 1. 0. 0. 0. 0. 2. 0. 1. 0. 0. 1. 2. 1. 0.]\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "print(p_new)\n",
    "print(p)\n",
    "print(p==p_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n",
      "compute_value_function at iteration # 16\n",
      "compute_value_function at iteration # 41\n",
      "compute_value_function at iteration # 43\n",
      "compute_value_function at iteration # 44\n",
      "compute_value_function at iteration # 45\n",
      "compute_value_function at iteration # 46\n",
      "compute_value_function at iteration # 46\n",
      "compute_value_function at iteration # 47\n",
      "compute_value_function at iteration # 48\n",
      "compute_value_function at iteration # 48\n",
      "policy-iteration converged at iteration # 9\n",
      "0.9\n",
      "compute_value_function at iteration # 19\n",
      "compute_value_function at iteration # 80\n",
      "compute_value_function at iteration # 83\n",
      "compute_value_function at iteration # 80\n",
      "compute_value_function at iteration # 84\n",
      "compute_value_function at iteration # 85\n",
      "compute_value_function at iteration # 86\n",
      "compute_value_function at iteration # 88\n",
      "compute_value_function at iteration # 92\n",
      "compute_value_function at iteration # 93\n",
      "policy-iteration converged at iteration # 9\n",
      "0.95\n",
      "compute_value_function at iteration # 20\n",
      "compute_value_function at iteration # 144\n",
      "compute_value_function at iteration # 151\n",
      "compute_value_function at iteration # 131\n",
      "compute_value_function at iteration # 139\n",
      "compute_value_function at iteration # 141\n",
      "compute_value_function at iteration # 143\n",
      "compute_value_function at iteration # 147\n",
      "compute_value_function at iteration # 155\n",
      "compute_value_function at iteration # 157\n",
      "policy-iteration converged at iteration # 9\n",
      "0.99\n",
      "compute_value_function at iteration # 22\n",
      "compute_value_function at iteration # 382\n",
      "compute_value_function at iteration # 399\n",
      "compute_value_function at iteration # 409\n",
      "compute_value_function at iteration # 334\n",
      "compute_value_function at iteration # 340\n",
      "compute_value_function at iteration # 346\n",
      "compute_value_function at iteration # 357\n",
      "compute_value_function at iteration # 377\n",
      "compute_value_function at iteration # 381\n",
      "compute_value_function at iteration # 386\n",
      "policy-iteration converged at iteration # 10\n"
     ]
    }
   ],
   "source": [
    "for lmbda in [0.8,0.9,0.95,0.99]:\n",
    "        print(lmbda)\n",
    "        policy_iterations(env,lmbda=lmbda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75, 0.77, 0.79, 0.81, 0.83, 0.85, 0.87, 0.89, 0.91, 0.93, 0.95,\n",
       "       0.97, 0.99])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Alpha = np.arange(0.75,1,0.02)\n",
    "Lmbda = np.arange(0.10,1,0.05)\n",
    "correct_train = np.zeros([len(Alpha), len(Lmbda)])\n",
    "correct_test = np.zeros([len(Alpha), len(Lmbda)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent(object):\n",
    "    def __init__(self,obs_n,act_n,learning_rate,gamma,e_greed):\n",
    "        self.obs_n = obs_n\n",
    "        self.act_n = act_n\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = e_greed\n",
    "        self.Q = np.zeros([env.nS,env.nA])\n",
    "    \n",
    "    def predict(self,state):\n",
    "        Q_list = self.Q[state,:]\n",
    "        max_q = np.max(Q_list)\n",
    "        action_list = np.where(Q_list==max_q)[0]\n",
    "        action = np.random.choice(action_list)\n",
    "        return action\n",
    "        \n",
    "    def sample(self,state):\n",
    "        if np.random.uniform(0,1) < (1.0-self.epsilon):\n",
    "            action = self.predict(state)\n",
    "        else:\n",
    "            action = np.random.choice(self.act_n)\n",
    "        return action\n",
    "    \n",
    "    def learn(self,state,action,reward,next_state,done):\n",
    "        predict_Q = self.Q[state,action]\n",
    "        if done:\n",
    "            target_Q = reward\n",
    "        else:\n",
    "            target_Q = reward + self.gamma * np.max(self.Q[next_state,:])\n",
    "        \n",
    "        self.Q[state,action] += self.lr * (target_Q - predict_Q)\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarsaAgent(object):\n",
    "    def __init__(self,obs_n,act_n,learning_rate,gamma,e_greed):\n",
    "        self.obs_n = obs_n\n",
    "        self.act_n = act_n\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = e_greed\n",
    "        self.Q = np.zeros([env.nS,env.nA])\n",
    "    \n",
    "    def predict(self,state):\n",
    "        Q_list = self.Q[state,:]\n",
    "        max_q = np.max(Q_list)\n",
    "        action_list = np.where(Q_list==max_q)[0]\n",
    "        action = np.random.choice(action_list)\n",
    "        return action\n",
    "        \n",
    "    def sample(self,state):\n",
    "        if np.random.uniform(0,1) < (1.0-self.epsilon):\n",
    "            action = self.predict(state)\n",
    "        else:\n",
    "            action = np.random.choice(self.act_n)\n",
    "        return action\n",
    "    \n",
    "    def learn(self,state,action,reward,next_state,next_action,done):\n",
    "        predict_Q = self.Q[state,action]\n",
    "        if done:\n",
    "            target_Q = reward\n",
    "        else:\n",
    "            target_Q = reward + self.gamma * self.Q[next_state,next_action]\n",
    "        self.Q[state,action] += self.lr * (target_Q - predict_Q)\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode_Sarsa(env,agent,is_render=False):\n",
    "    total_steps = 0\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "    action = agent.sample(state)\n",
    "    while True:\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_action = agent.sample(next_state)\n",
    "        # update Q table\n",
    "        agent.learn(state,action,reward,next_state,next_action,done)\n",
    "        # s <- s'   a <- a'\n",
    "        state = next_state\n",
    "        action = next_action\n",
    "        total_steps += 1\n",
    "        total_reward += reward\n",
    "        if is_render:\n",
    "            env.render()\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward,total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode_Q(env,agent,is_render=False):\n",
    "    total_steps = 0\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        action = agent.sample(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        agent.learn(state,action,reward,next_state,done)\n",
    "        state = next_state\n",
    "        total_steps += 1\n",
    "        total_reward += reward\n",
    "        if is_render:\n",
    "            env.render()\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward,total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_episode(env,agent,is_render=False):\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "    \n",
    "    while True:\n",
    "        action = agent.predict(state)\n",
    "        next_state,reward,done,_ = env.step(action)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        if is_render:\n",
    "            time.sleep(0.5)\n",
    "            env.render()\n",
    "        if done:\n",
    "            #print('test_reward', total_reward)\n",
    "            break\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0\n",
      "36.0\n",
      "186.0\n",
      "238.0\n",
      "332.0\n",
      "301.0\n",
      "303.0\n",
      "312.0\n",
      "310.0\n",
      "292.0\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "S_agent = SarsaAgent(obs_n = env.nS, act_n = env.nA, learning_rate = 0.1, gamma = 0.9, e_greed = 0.1)  \n",
    "reward_his = []\n",
    "for episode in range(10000):\n",
    "    ep_reward, ep_steps = run_episode(env,S_agent)\n",
    "    reward_his.append(ep_reward)\n",
    "    #print(\"Episode %s: steps = %s, reward = %1.f\"%(episode, ep_steps, ep_reward))\n",
    "for i in range(10):\n",
    "    print(sum(reward_his[i*1000:(i+1)*1000-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "10.0\n",
      "121.0\n",
      "249.0\n",
      "277.0\n",
      "257.0\n",
      "285.0\n",
      "278.0\n",
      "312.0\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "Q_agent = QAgent(obs_n = env.nS, act_n = env.nA, learning_rate = 0.1, gamma = 0.9, e_greed = 0.1)  \n",
    "reward_his = []\n",
    "for episode in range(10000):\n",
    "    ep_reward, ep_steps = run_episode_Q(env,Q_agent)\n",
    "    reward_his.append(ep_reward)\n",
    "    #print(\"Episode %s: steps = %s, reward = %1.f\"%(episode, ep_steps, ep_reward))\n",
    "for i in range(10):\n",
    "    print(sum(reward_his[i*1000:(i+1)*1000-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "test_reward 1.0\n",
      "test_reward 0.0\n",
      "test_reward 0.0\n",
      "0.259\n"
     ]
    }
   ],
   "source": [
    "test_reward = 0\n",
    "for i in range(1000):\n",
    "    test_reward += test_episode(env,S_agent)\n",
    "print(test_reward/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RL_Q(env, iterations=2000, alpha = 0.8, lmbda = 0.99):\n",
    "    # alpha is learning rate; lmbda is discount factor\n",
    "    # initial Q table, dim is [s,a]\n",
    "    Q_all = np.zeros([env.nS,env.nA])\n",
    "    rList = []\n",
    "    for i in range(iterations):\n",
    "        s = env.reset()\n",
    "        rAll = 0\n",
    "        d = False\n",
    "        for i in range(1000):\n",
    "            a = np.argmax(Q_all[s,:] + np.random.randn(1,env.nA) * (1./(i+1)))\n",
    "            s1, r, d, _ = env.step(a)\n",
    "            # update Q table\n",
    "            # Q learning\n",
    "            Q_all[s,a] = Q_all[s,a] + alpha*(r+lmbda*np.max(Q_all[s1,:]) - Q_all[s,a])\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            if d:\n",
    "                break\n",
    "        rList.append(rAll)\n",
    "    \n",
    "    return Q_all, sum(rList)/iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()  \n",
    "Q, avg_reward = RL_Q(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.099"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sarsa选取的是一种保守的策略，他在更新Q值的时候已经为未来规划好了动作，对错误和死亡比较敏感。 \n",
    "而Q-learning每次在更新的时候选取的是最大化Q的方向，而当下一个状态时，再重新选择动作，\n",
    "Q-learning是一种鲁莽、大胆、贪婪的算法，对于死亡和错误并不在乎"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
